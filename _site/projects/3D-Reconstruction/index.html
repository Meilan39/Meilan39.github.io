<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>文化遺産の保存と共有のための３次元復元（英語版）- 共同研究 - Meilan Muto Portfolio</title>
<meta name="description" content="Some would say that we are currently living through the information age, a universal turning point for civilization which has the potential to rival the agricultural and industrial revolution in terms of societal impact. One important aspect of the information age is that it has enabled the sharing of information with far less work and with far more accessibility than in the past. The invention of the internet and its advancement as a global exchange for information has undoubtedly raised the bar for the collective intelligence of mankind. Increasing the amount of information that can be stored and transferred has been a field of constant research in the past decades, but there comes a point where the medium of information exchange must be updated to allow for further growth. Recent advancements in fields such as VR (Virtual Reality) have begun to foreshadow the next stage of the information revolution, beginning with a new medium to share information with a level of immersion that can hardly be imagined today.">


  <meta name="author" content="武藤皐蘭">
  
  <meta property="article:author" content="武藤皐蘭">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Meilan Muto Portfolio">
<meta property="og:title" content="文化遺産の保存と共有のための３次元復元（英語版）- 共同研究">
<meta property="og:url" content="http://localhost:4000/projects/3D-Reconstruction/">


  <meta property="og:description" content="Some would say that we are currently living through the information age, a universal turning point for civilization which has the potential to rival the agricultural and industrial revolution in terms of societal impact. One important aspect of the information age is that it has enabled the sharing of information with far less work and with far more accessibility than in the past. The invention of the internet and its advancement as a global exchange for information has undoubtedly raised the bar for the collective intelligence of mankind. Increasing the amount of information that can be stored and transferred has been a field of constant research in the past decades, but there comes a point where the medium of information exchange must be updated to allow for further growth. Recent advancements in fields such as VR (Virtual Reality) have begun to foreshadow the next stage of the information revolution, beginning with a new medium to share information with a level of immersion that can hardly be imagined today.">







  <meta property="article:published_time" content="2025-05-28T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/projects/3D-Reconstruction/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Meilan Muto Portfolio Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Meilan Muto Portfolio
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/"
                
                
              >Home</a>
            </li><li class="masthead__menu-item">
              <a
                href="/projects/"
                
                
              >Projects</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">武藤皐蘭</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>東京農工大学 工学部 知能情報システム工学科 学部２年生</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">埼玉県</span>
        </li>
      

      
        
          
            <li><a href="mailto:muto.meilan@gmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/Meilan39" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://jp.linkedin.com/in/meilan-muto-3b8a01355" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="文化遺産の保存と共有のための３次元復元（英語版）- 共同研究">
    <meta itemprop="description" content="Some would say that we are currently living through the information age, a universal turning point for civilization which has the potential to rival the agricultural and industrial revolution in terms of societal impact. One important aspect of the information age is that it has enabled the sharing of information with far less work and with far more accessibility than in the past. The invention of the internet and its advancement as a global exchange for information has undoubtedly raised the bar for the collective intelligence of mankind. Increasing the amount of information that can be stored and transferred has been a field of constant research in the past decades, but there comes a point where the medium of information exchange must be updated to allow for further growth. Recent advancements in fields such as VR (Virtual Reality) have begun to foreshadow the next stage of the information revolution, beginning with a new medium to share information with a level of immersion that can hardly be imagined today.">
    <meta itemprop="datePublished" content="2025-05-28T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/projects/3D-Reconstruction/" itemprop="url">文化遺産の保存と共有のための３次元復元（英語版）- 共同研究
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Some would say that we are currently living through the information age, a universal turning point for civilization which has the potential to rival the agricultural and industrial revolution in terms of societal impact. One important aspect of the information age is that it has enabled the sharing of information with far less work and with far more accessibility than in the past. The invention of the internet and its advancement as a global exchange for information has undoubtedly raised the bar for the collective intelligence of mankind. Increasing the amount of information that can be stored and transferred has been a field of constant research in the past decades, but there comes a point where the medium of information exchange must be updated to allow for further growth. Recent advancements in fields such as VR (Virtual Reality) have begun to foreshadow the next stage of the information revolution, beginning with a new medium to share information with a level of immersion that can hardly be imagined today.</p>

<p>To support an advancement of the information medium, a simple and accessible method to convert seamlessly between objects in the real world and those in the virtual world becomes of dire demand. 3D reconstruction is a field of computer vision that involves the “reconstruction” of 3D objects from 2D pictures. Examples of uses for 3D reconstruction include, generating 3D models of museum artifacts to share with the world, or increasing the reliability and efficiency of self-driving cars by generating models of the environment with greater accuracy.</p>

<h1 id="2-camera-calibration">2. Camera Calibration</h1>
<h2 id="21-motivation">2.1. Motivation</h2>

<p>Camera calibration is an important step in the 3D-reconstruction model and the primary focus of this research report. The role of camera calibration is to parameterize the internal properties and position/orientation of a given camera from the images that it takes. Additionally, camera lenses are necessarily flawed in their construction, and have varying degrees of optical distortion that can be modeled and accounted for through camera calibration. This research report details an implementation of the OpenCV (Open Source Computer Vision Library) camera calibration model, which involves matching 3D object points to their corresponding 2D image points, and optimizing several parameters based on the reprojection error.
 </p>
<h2 id="22-principles-of-camera-calibration">2.2. Principles of Camera Calibration</h2>

<h3 id="221-pin-hole-camera-model">2.2.1. Pin-hole Camera Model</h3>

<p>The pin-hole camera model is a theoretical camera model that enables a simple geometrical interpretation of the transformation between a world coordinate and its corresponding pixel coordinate. The pin-hole camera model consists of a sensor, a box, and a pinhole, and they are arranged as shown in figure 1. The sensor is represented by the plane between the camera and the object, and the pinhole is represented by the hole where the orange rays intersect.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure1.png" alt="figure1" title="figure1" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 1: A graphic representation of the pin-hole model [1] </div>
<p><br /></p>

<h3 id="222-homogeneous-coordinates">2.2.2. Homogeneous Coordinates</h3>
<p>The homogeneous coordinate system is a coordinate system that is used primarily in computer vision and computer graphics, for its ability to express affine transformation and projective transformations with relative ease. This is because homogeneous coordinates can express points at infinity with finite coordinates [2]. In other words, they represent a ray through space with a single n+1 dimensional vector.
One would turn a n-dimensional vector into a homogeneous vector by appending a 1 as follows:</p>

\[\left[\begin{matrix}X\\Y\\Z\\\end{matrix}\right]\longrightarrow\left[\begin{matrix}X\\Y\\Z\\1\\\end{matrix}\right]\]

<p>Similarly, one would turn a (n+1)-dimensional homogeneous vector into a n-dimensional Cartesian vector by dividing all elements by the last element and removing the last element as follows:</p>

\[\left[\begin{matrix}X\\Y\\Z\\\end{matrix}\right]\longrightarrow\left[\begin{matrix}\frac{X}{Z}\\\frac{Y}{Z}\\\end{matrix}\right]\]

<h3 id="223-extrinsic-matrix">2.2.3. Extrinsic Matrix</h3>

<p>The extrinsic matrix of a picture represents the affine transformation of the 3D world coordinate system to the 3D camera coordinate system. The name extrinsic matrix implies that the values contained within the matrix are extrinsic to the camera and represents how the camera is situated in space.</p>

<p>Any camera’s pose, i.e. its position and orientation, can be represented by 3-dimensional rotation and translation. The transformation of a 3D homogeneous world coordinate $\left(X_w,Y_w,Z_w,\ 1\right)$ to its 3D camera coordinate $\left(X_c,Y_c,Z_c\right)$ is given by,</p>

\[\left[\begin{matrix}X_c\\Y_c\\Z_c\\\end{matrix}\right]=\left[\begin{matrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\\end{matrix}\right]\left[\begin{matrix}X_w\\Y_w\\Z_w\\1\\\end{matrix}\right].\]

<table>
  <tbody>
    <tr>
      <td>The extrinsic matrix is thus a joint rotation and transformation matrix $\left[R\middle</td>
      <td>t\right]$ given by equation (1), where $R$ is the rotational matrix and $t$ is the translation matrix representing the change in basis from the world coordinate to the camera coordinate.</td>
    </tr>
  </tbody>
</table>

\[\begin{equation}
\left[R\middle| t\right]=\left[\begin{matrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\\end{matrix}\right]
\end{equation}\]

<h3 id="224-intrinsic-matrix">2.2.4. Intrinsic Matrix</h3>

<p>The Intrinsic matrix of a given picture represents the transformation of the 3D camera coordinate system to the 2D pixel coordinate system. The name “intrinsic matrix” implies that the values within this matrix (intrinsic parameters) are characteristics of the camera that took the photo, and not dependent on external factors such as the location or orientation of the camera in the world coordinate system.</p>

<p>Figure 2 depicts the relationship between a 3D world coordinate $\left(X_w,Y_w,Z_w\right)$ and its corresponding 2D pixel coordinate $\left(u,v\right)$ in the pin-hole camera model.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure2.png" alt="figure2" title="figure2" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 2: the relationship between a 3D world coordinate and a 2D pixel coordinate [3] </div>
<p><br /></p>

<p>When the principle point of the image sensor (i.e. the center of the image sensor) is given by $\left(c_x,c_y\right)$, and the 3D world coordinate is defined by the camera coordinate system, the 2D pixel coordinate $\left(u,v\right)$ of a given 3D camera coordinate $\left(X_c,Y_c,Z_c\right)$ is described by</p>

\[\begin{bmatrix} u \\ v \end{bmatrix}
= 
\begin{bmatrix} f_x \frac{X_c}{Z_c} + c_x  \\  f_y \frac{Y_c}{Z_c} + c_y \end{bmatrix}\]

<p>represented in practice as a matrix product of the form</p>

\[\left[\begin{matrix}u\\v\\1\\\end{matrix}\right]=\left[\begin{matrix}f_x&amp;0&amp;c_x\\0&amp;f_y&amp;c_y\\0&amp;0&amp;1\\\end{matrix}\right]\left[\begin{matrix}X_c\\Y_c\\Z_c\\\end{matrix}\right],\]

<p>where $\left[\begin{matrix}u&amp;v&amp;1\end{matrix}\right]^\top$ is a homogeneous representation of $\left[\begin{matrix}u&amp;v\end{matrix}\right]^\top$.</p>

<p>The intrinsic matrix $A$ is thus defined by matrix (2), where $f_x$ and $f_y$ represent the camera’s focal length in the x-direction and y-direction respectively, and $c_x$ and $c_y$ represent the camera’s principle point in x-coordinates and y-coordinates respectively.</p>

\[\begin{equation}
A=\left[\begin{matrix}f_x&amp;0&amp;c_x\\0&amp;f_y&amp;c_y\\0&amp;0&amp;1\\\end{matrix}\right]
\end{equation}\]

<h3 id="225-optical-distortion">2.2.5. Optical Distortion</h3>

<p>In camera calibration, it is typical to consider two types of optical distortion: radial and tangential distortions. Radial and tangential distortions are primarily caused by manufacturing inconsistencies that alter the shape of the lens. The effects of radial and tangential distortions are mitigated in the calibration process by altering the intrinsic matrix equation into equation (3).</p>

\[\begin{equation}
\left[\begin{matrix}u\\v\\\end{matrix}\right]=\left[\begin{matrix}f_xx^{\prime\prime}+c_x\\f_yy^{\prime\prime}+c_y\\\end{matrix}\right]
\end{equation}\]

<p>With</p>

\[\left[\begin{matrix}x^{\prime\prime}\\y^{\prime\prime}\\\end{matrix}\right] = \left[\begin{matrix}x^\prime\frac{1+k_1r^2+k_2r^4+k_3r^6}{1+k_4r^2+k_5r^4+k_6r^6}+2p_1x^\prime y^\prime+p_2\left(r^2+2x^{\prime2}\right)+s_1r^2+s_2r^4\\y^\prime\frac{1+k_1r^2+k_2r^4+k_3r^6}{1+k_4r^2+k_5r^4+k_6r^6}+p_1\left(r^2+2y^{\prime2}\right)+2p_2x^\prime y^\prime+s_3r^2+s_4r^4\\\end{matrix}\right],\]

\[\begin{align*}
r^2 &amp;= x^{\prime2}+y^{\prime2}, \\
\left[x^\prime,y^\prime\right] &amp;= \left[\frac{X_c}{Z_c},\frac{Y_c}{Z_c}\right].
\end{align*}\]

<p>Where, $k_1, k_2, k_3, k_4, k_5,$ and $k_6$ are the radial distortion coefficients; $p_1$ and $p_2$ are tangential distortions coefficients; and $s_1, s_2, s_3,$ and $s_4$ are the thin prism distortion coefficients.</p>

<h3 id="226-camera-matrix">2.2.6. Camera Matrix</h3>

<p>The final camera matrix is a matrix multiplication of the intrinsic and extrinsic matrices, and represented in equation (4), where s is a scale factor that adjusts for the unknown scale factor of the projective transformation.</p>

\[\begin{equation}
s\left[\begin{matrix}u\\v\\1\\\end{matrix}\right]=\left[\begin{matrix}f_x&amp;0&amp;c_x\\0&amp;f_y&amp;c_y\\0&amp;0&amp;1\\\end{matrix}\right]\left[\begin{matrix}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_x\\r_{21}&amp;r_{22}&amp;r_{23}&amp;t_y\\r_{31}&amp;r_{32}&amp;r_{33}&amp;t_z\\\end{matrix}\right]\left[\begin{matrix}X_w\\Y_w\\Z_w\\1\\\end{matrix}\right]
\end{equation}\]

<p>Equation (4) can also be represented in the form,</p>

\[sp=A\left[R\middle| t\right]P_w,\]

<p>where $p$ is the pixel coordinate and $P_w$ is the world coordinate both represented in homogeneous coordinates.</p>

<h2 id="23-camera-calibration-from-a-checkerboard-pattern">2.3. Camera Calibration from a Checkerboard Pattern</h2>

<p>The standard method for camera calibration, and the method outlined in OpenCV’s documentation, involves taking pictures of a well-defined geometric pattern such as a chessboard and finding a transformation to map the 2-dimensional image points (pixel coordinates of the corners) to their known 3-dimensional object points (world coordinates of the corners).</p>

<p>The checkerboard pattern used in this method is shown in figure 3, which has 7 by 10 inner corners. The pattern was obtained as a pdf file from Mark Hedley Jones’s Calibration Checkerboard Collection [4] and printed onto standard A4 sized printing paper. 20 images were then taken from various angles with the Canon EOS R100 mounted with the Canon Lens EF 28 mm and the EF-EOS mount adapter, at a fixed focal length.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure3.svg" alt="figure3" title="figure3" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 3: 8 by 11 checkerboard pattern with 7 by 10 inner corners [4]  

black and white when printed on paper </div>
<p><br /></p>

<p>Program 1 is a python script that was written with the OpenCV documentation [5] as reference, and takes in a directory of images in order to derive their image points and object points.</p>

<div style="text-align: center;"> Program 1: “findCorners” function definition </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">findCorners</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">show</span><span class="p">):</span>
    <span class="c1"># assign object points
</span>    <span class="n">objp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">dimensions</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">objp</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mgrid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">dimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">:</span><span class="n">dimensions</span><span class="p">[</span><span class="mi">1</span><span class="p">]].</span><span class="n">T</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">objpoints</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 3d objectpoints
</span>    <span class="n">imgpoints</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 2d image points
</span>    <span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># names of the image file
</span>
    <span class="c1"># get /*.JPG from specified directory
</span>    <span class="n">images</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="n">glob</span><span class="p">(</span><span class="n">directory</span> <span class="o">+</span> <span class="s">'/*.JPG'</span><span class="p">)</span>  <span class="c1"># Specify the path to your images
</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
        <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

        <span class="c1"># Find chessboard corners
</span>        <span class="n">ret</span><span class="p">,</span> <span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">findChessboardCorners</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="c1"># If found, add object points, image points
</span>        <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">objpoints</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">objp</span><span class="p">)</span>
            <span class="n">imgpoints</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">corners</span><span class="p">)</span>
            <span class="n">names</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>

        <span class="c1"># display corners if specified
</span>        <span class="k">if</span> <span class="n">show</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">drawChessboardCorners</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">corners</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
            <span class="n">cv2</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="s">'img'</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
            <span class="n">cv2</span><span class="p">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>  
    <span class="k">return</span> <span class="n">names</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span>
</code></pre></div></div>

<p>The function in program 1, searches through a given directory for files with the “.JPG” extension and runs them through an OpenCV function called “findChessboardCorners”. This function effectively maps the known 3D coordinates of the chessboard corners to the 2D coordinate that they occupy in the given image. If corners are found, their pixel coordinate and world coordinates as well as the name of the image to which they belong, are stored in an array and are returned. Additionally, this information is passed to the “drawChessboardCorners” function where the detected chessboard corners are indicated on the image by dots, and their connections by lines. Figure 4 shows an example of a sample image (left) and its corresponding corner-indicated image (right).</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure4.png" alt="figure4" title="figure4" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 4: A sample image (left) and its corresponding corner-indicated image (right) </div>
<p><br /></p>

<p>The next step is to utilize the derived object points and image points to calculate the camera matrix. Program 2 is a python script that utilizes the OpenCV “calibrateCamera” function to take object and image points as inputs and produces several matrix arrays which are combined to make the camera matrix. The “savePoint” and “saveCalib” functions are used to save the image points, object points, and resulting camera matrices as NumPy files. Their definitions are shown in program 3.</p>

<div style="text-align: center;"> Program 2: camera calibration from image points and object points </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Number of internal corners in the chessboard
</span><span class="n">DIMENSIONS</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Current Directory 
</span><span class="n">DIRECTORY</span> <span class="o">=</span> <span class="s">'batch1'</span>

<span class="c1"># get object points and image points
</span><span class="n">names</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span> <span class="o">=</span> <span class="n">findCorners</span><span class="p">(</span><span class="n">DIRECTORY</span><span class="p">,</span> <span class="n">DIMENSIONS</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Camera calibration
</span><span class="n">ret</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">calibrateCamera</span><span class="p">(</span><span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">,</span> <span class="n">RESOLUTION</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="c1"># save points and calibration
</span><span class="n">savePoint</span><span class="p">(</span><span class="n">DIRECTORY</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">)</span>
<span class="n">saveCalib</span><span class="p">(</span><span class="n">DIRECTORY</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">)</span>
</code></pre></div></div>

<div style="text-align: center;"> Program 3: “saveCalib” and “savePoint” function definitions </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">saveCalib</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">directory</span> <span class="o">+</span> <span class="s">'/calib.npy'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">mtx</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">savePoint</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">directory</span> <span class="o">+</span> <span class="s">'/point.npy'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">)</span>
</code></pre></div></div>

<p>An example of a calibration result is shown in figure 5. The x focal length and y focal length were calculated by converting the relevant intrinsic matrix component with the image sensor’s dimensions and pixel resolution, effectively converting the focal length given in pixel units to be given in mm units.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure5.png" alt="figure5" title="figure5" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 5: the resulting intrinsic and extrinsic matrices of a camera calibration </div>
<p><br /></p>

<h2 id="24-visualizing-distortion-with-reprojection-error">2.4. Visualizing Distortion with Reprojection Error</h2>

<p>To estimate the accuracy of the calibration, the reprojection error is calculated. In fact, the OpenCV “calibrateCamera” function optimizes intrinsic, extrinsic, and distortion parameters by minimizing the reprojection error. By disabling the distortion parameter optimization in the “calibrateCamera” function and plotting the reprojection error, the effects of lens distortion can be visualized.</p>

<p>Reprojection error is calculated by taking the mean pixel distance between the 3D object points of the chessboard corners and the hypothetical projection of those corners to the image plane using the newly calculated camera matrix. By comparing where the corners are projected and where the corners should be, the accuracy of the transformation can be inferred. To obtain a visual representation of this process, program 4, based on the OpenCV documentation [5], draws exaggerated lines on the image to give a relative representation of reprojection error.</p>

<div style="text-align: center;"> Program 4: “reproject” function definition </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reproject</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">show</span><span class="p">):</span>
    <span class="n">mean_error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">line_scaler</span> <span class="o">=</span> <span class="mi">50</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objpoints</span><span class="p">)):</span>
         <span class="n">imgpoints2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">projectPoints</span><span class="p">(</span><span class="n">objpoints</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">rvecs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tvecs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
         <span class="n">error</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">imgpoints</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">imgpoints2</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">NORM_L2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">imgpoints2</span><span class="p">)</span>
         <span class="n">mean_error</span> <span class="o">+=</span> <span class="n">error</span>

         <span class="k">if</span> <span class="n">show</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="c1"># Draw original and reprojected points
</span>            <span class="n">reprojected_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">pt_orig</span><span class="p">,</span> <span class="n">pt_proj</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">imgpoints</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">imgpoints2</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
                <span class="n">pt_len</span> <span class="o">=</span> <span class="p">[</span> <span class="n">pt_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">pt_proj</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">line_scaler</span><span class="p">,</span> 
<span class="n">pt_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">pt_proj</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">line_scaler</span> <span class="p">]</span>
                <span class="n">cv2</span><span class="p">.</span><span class="n">circle</span><span class="p">(</span><span class="n">reprojected_img</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">cv2</span><span class="p">.</span><span class="n">line</span><span class="p">(</span><span class="n">reprojected_img</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">pt_orig</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> 
<span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">pt_len</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">pt_len</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>

            <span class="n">cv2</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="s">'Reprojected'</span><span class="p">,</span> <span class="n">reprojected_img</span><span class="p">)</span>
            <span class="n">cv2</span><span class="p">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">cv2</span><span class="p">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mean_error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">objpoints</span><span class="p">)</span>
</code></pre></div></div>

<p>Looping through each of the images, the OpenCV function “reprojectPoints” is used to project object points to the image plane, and the “norm” function is used to calculate the absolute distance between the image points and the projected points. Circles are drawn around the image points, and lines are drawn to the projected points, with their lengths scaled by 50 pixels to make them easily visible.</p>

<p>The two images shown in figure 6, are examples of images that have been calibrated with distortion correction disabled. The image on the left is an example of a good reprojection, and a particularly good example of positive radial distortion, a type of lens aberration where the image seems to bulge towards the lens and points are projected farther from the center than they should be. The lines in the left image indicate that the projected points were generally closer to the center than the actual image points, which would fall in line with the properties of positive radial distortion. Additionally, radial distortion has a tendency for the reprojection errors to become larger the farther they are from the center of the image, which can generally be seen in both images.</p>

<div class="image-row-two">
  <div class="image-column-two">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure6left.png" alt="" />
  </div>
  <div class="image-column-two">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure6right.png" alt="" />
  </div>
</div>

<div style="text-align: center;"> Figure 6: Reprojection error with distortion correction disabled </div>
<p><br /></p>

<p>Enabling the “calibrateCamera” distortion correction, reprojecting the images again gives the images shown in figure 7. It can be noted that the reprojection errors have been significantly reduced.</p>

<div class="image-row-two">
  <div class="image-column-two">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure7left.png" alt="" />
  </div>
  <div class="image-column-two">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure7right.png" alt="" />
  </div>
</div>

<div style="text-align: center;"> Figure 7: Reprojection error with distortion correction enabled </div>
<p><br /></p>

<h2 id="25-plotting-camera-pose-from-calibration-parameters">2.5. Plotting Camera Pose from Calibration Parameters</h2>

<p>To ensure that there are no obvious issues with the extrinsic parameters, it is useful to derive the camera pose from the extrinsic parameters and plot them in a 3D coordinate system. The derivation of this transformation begins with a reconfirmation of the relationship between the world coordinate system and the camera coordinate system in the current model. The extrinsic matrix is a joint rotation-translation matrix which is broken down as follows,</p>

\[\left[\begin{matrix}X_c\\Y_c\\Z_c\\\end{matrix}\right]=\left[\begin{matrix}r_{11}&amp;r_{12}&amp;r_{13}\\r_{21}&amp;r_{22}&amp;r_{23}\\r_{31}&amp;r_{32}&amp;r_{33}\\\end{matrix}\right]\left[\begin{matrix}X_w\\Y_w\\Z_w\\\end{matrix}\right]+\left[\begin{matrix}t_x\\t_y\\t_z\\\end{matrix}\right].\]

<p>For simplicity, the rotation matrix is represented as R and the translation vector is represented as t. The world coordinate is the unknown, so it is rewritten as P_W, and the camera coordinate is rewritten as the camera’s position in the camera coordinate system, the origin, and thus the zero vector 0. Therefore, the new expression is written as</p>

\[\mathbf{0}=RP_W+t,\]

<p>Which is simplified to</p>

\[P_W=-R^{-1}t.\]

<p>Here, the fact that for a rotation matrix $R$, its inverse $R^{-1}$ and its transpose $R^\top$ are the same matrix is used to further simplify the above equation into expression (5).</p>

\[\begin{equation} P_W=-R^\top t \end{equation}\]

<p>With expression (5), the camera pose for a given extrinsic matrix can be calculated trivially and plotted with an extension such as matplotlib. Program 5 shows the implementation of this method in python.</p>

<div style="text-align: center;"> Program 5: python file to plot camera pose on a 3D matplotlib graph </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">mpl_toolkits.mplot3d.art3d</span> <span class="k">as</span> <span class="n">art3d</span>

<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Rectangle</span>
<span class="kn">from</span> <span class="nn">matplotlib.transforms</span> <span class="kn">import</span> <span class="n">Affine2D</span>

<span class="kn">from</span> <span class="nn">definitions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">BOARD</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="n">names</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span> <span class="o">=</span> <span class="n">loadCalib</span><span class="p">(</span><span class="s">'batch1'</span><span class="p">)</span>

<span class="c1"># instantiate figure
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="c1"># instantiate axis
</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_zlim</span><span class="p">([</span><span class="mi">0</span>  <span class="p">,</span> <span class="mi">40</span><span class="p">])</span>

<span class="c1"># Draw the checkerboard on the floor
</span><span class="n">p</span> <span class="o">=</span> <span class="n">Rectangle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">BOARD</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">BOARD</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">art3d</span><span class="p">.</span><span class="n">pathpatch_2d_to_3d</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s">"z"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rvecs</span><span class="p">)):</span>
    <span class="c1"># rodrigues rotation vector to rotation matrix
</span>    <span class="n">R</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">Rodrigues</span><span class="p">(</span><span class="n">rvecs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># find the camera center
</span>    <span class="n">C</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">R</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># print the basis vectors of the camera coordinate system
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">C</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">C</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">R</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">"green"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">C</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">R</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">R</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>
    
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The resulting camera pose graph can be seen in figure 8, where the basis vector in blue represents the z-axis of the camera coordinate system, and thus the direction of the camera in 3D space. Additionally, the blue square at the bottom of the graph represents the chessboard pattern in 3D space.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure8.png" alt="figure8" title="figure8" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 8: camera pose derived from the 14 extrinsic parameters of batch 1 </div>
<p><br /></p>

<p>It can be observed that the camera poses are all pointing towards the chessboard pattern, and further examination reveals that their positions and orientations seem to match the expected camera poses of the individual images. The first batch of sample images has been highlighted in detail within this report, but other samples such as the 3 samples shown in figure 9, were also taken to determine the quantitative effects of altering the focal lengths on the camera’s intrinsic matrix. As the focal lengths increase from left to right in figure 9, it was necessary to take the images farther from the checkerboard pattern in order to retain focus, which seems to be depicted with great accuracy in the resulting camera poses.</p>

<div class="image-row-three">
  <div class="image-column-three">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure9left.png" alt="" />
  </div>
  <div class="image-column-three">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure9mid.png" alt="" />
  </div>
  <div class="image-column-three">
    <img src="/assets/2025-5-28-3D-Reconstruction/figure9right.png" alt="" />
  </div>
</div>

<div style="text-align: center;"> Figure 9: camera pose derived from extrinsic parameters with focal length 0.5, 0.75, 1.0 from left to right </div>
<p><br /></p>

<h1 id="3-3d-reconstruction-from-video">3. 3D Reconstruction from Video</h1>
<h2 id="31-motivation">3.1. Motivation</h2>

<p>The need to convert seamlessly between the real and virtual world is not just a concern for professionals, and can be expected to play an important role in the everyday lives of ordinary people. However, a challenge arises in the difficulty of collecting sample data, as current 3D reconstruction techniques require specific and expensive equipment that would be unfeasible to implement at the consumer level. Further advancement of 3D reconstruction methods must also entail an increase in their accessibility and ease of use.</p>

<p>One implication of technological advancements in recent years is that, although the ordinary person may not have a high-end digital camera, the ordinary person is increasingly likely to have a smartphone. Additionally, videos can be seen as a discrete array of images. Thus, a new method to conduct 3D reconstruction by extracting the non-blurry frames from a video of an object taken with a smartphone, is considered in this report. Considering that videos generally have around 30 to 60 frames per second, the difference between adjacent frames is typically insignificant in the context of SFM (Structure from Motion). Additionally, videos use auto focus to maintain focus on a subject, but their control systems are typically based off of feedback systems and have latencies and result in an uneven distribution of focus throughout the frames of a video. A consequence of this is that most frames in a video with motion are blurry. Although this is not a concern when played back with real time speed, a blurry image for 3D reconstruction implies that the resulting point cloud contains uncertainty. Depending on its severity, the algorithm may decide that the point is too uncertain to even plot. Although 3D reconstruction is not conducted in this report, a method to extract the non-blurry frames from an image using Laplacian blur detection is explored.</p>

<h2 id="32-sampling-images-from-a-video">3.2. Sampling Images from a Video</h2>

<p>There are two main methods for detecting if a given image is blurry. The first method involves comparing the variance of images that have been converted to gray scale and ran through a Laplacian filter. Details of this method are given in section 2.2.1. The second method involves comparing the magnitude spectrums of images that have undergone a Fourier transform, and determining the ratio of high to low-frequency components of the image. This method is not discussed or implemented in this research.</p>

<h3 id="321-laplacian-variance-method">3.2.1. Laplacian Variance Method</h3>

<p>The Laplacian filter is a convolution filter that can be described as a discrete second-order derivative of an input image. The Laplacian filter is typically used as a method to quantitatively describe regions of rapid intensity change in an image, making it an ideal fit for edge-detection applications. Because the primary distinction between blurry and non-blurry images are the sharpness of their edges, the Laplacian filter can also be used to describe the relative blurriness of an image. In particular, the variance of an image that has been passed through the Laplacian filter can be used to compare the level of blur between similar images. It is important to consider however, that the Laplacian variance is only meaningful when taken with respect to images of similar composition, as this method is, for instance, incapable of distinguishing between sharp but monotone images and unsharp but colorful images.</p>

<p>The Laplacian filter method described in the OpenCV documentation [6] uses a 3 by 3 convolution kernel of the form</p>

\[\left[\begin{matrix}0&amp;1&amp;0\\1&amp;-4&amp;1\\0&amp;1&amp;0\\\end{matrix}\right].\]

<p>The implementation is given in program 6. The “GetVar” function takes an image as an input, converts the image to gray scale and returns Laplacian variance by using the OpenCV “cv2.Laplacian()” function and calling the NumPy “var()” method on the resulting NumPy array. The “GetLaplacians” function takes the path to a video and an output folder as inputs and calculates the Laplacian variance of each frame in a video, storing the resulting values in a NumPy file.</p>

<div style="text-align: center;"> Program 6: “GetVar” and “GetLaplacian” function definitions </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">GetVar</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
    <span class="n">laplacian_var</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">Laplacian</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">CV_64F</span><span class="p">).</span><span class="n">var</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">laplacian_var</span>

<span class="k">def</span> <span class="nf">GetLaplacians</span><span class="p">(</span><span class="n">video_path</span><span class="p">,</span> <span class="n">output_folder</span><span class="p">):</span>
    <span class="n">laplacians</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cap</span><span class="p">.</span><span class="n">isOpened</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: Could not open video </span><span class="si">{</span><span class="n">video_path</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">laplacians</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">GetVar</span><span class="p">(</span><span class="n">frame</span><span class="p">))</span>
    <span class="n">cap</span><span class="p">.</span><span class="n">release</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_folder</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">np</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">laplacians</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="322-filtering-laplacian-variance-for-optimal-image-selection">3.2.2. Filtering Laplacian Variance for Optimal Image Selection</h3>

<p>Program 6 produces a NumPy array that contains the Laplacian variance values of each frame in a video. The next step is to find a method to optimally select images based on these values. As mentioned previously, the Laplacian variance values themselves have no inherent meaning, and only have meaning relative to the values of adjacent frames. Thus, simply selecting the top N highest Laplacian variance values from a video will not necessarily produce good results. Additionally, the images that are selected in this process are ultimately used in SFM, where adjacent frames should be relatively near each other in order to effectively find correspondences between them. It is thus, also important to consider the interval of the selected frames for optimal results. The three main filters proposed in this research are the Decay Filter, the Moving Maximum Filter, and the Local Maxima Filter.</p>

<h3 id="323-decay-filter">3.2.3. Decay Filter</h3>

<p>Program 7 shows the implementation of the decay filter. The decay filter takes a threshold and a decay constant as inputs. The search begins by setting the moving threshold to the input threshold. Looping through all elements of the array, while the current value does not meet the threshold, the threshold is decreased by the decay constant, and the next value is checked. When a value that meets the threshold is found, the moving threshold is set back to the initial input threshold, and the process is repeated.</p>

<div style="text-align: center;"> Program 7: “DecayFilter” function definition </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">DecayFilter</span><span class="p">(</span><span class="n">laplacians</span><span class="p">,</span> <span class="n">blur_threshold</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">decay_per_cycle</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">array</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">blur_threshold</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">laplacians</span><span class="p">)):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">threshold</span><span class="o">&lt;</span><span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span> 
            <span class="n">array</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">blur_threshold</span> <span class="k">if</span> <span class="n">threshold</span><span class="o">&lt;</span><span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="n">threshold</span> <span class="o">-</span> <span class="n">decay_per_cycle</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
</code></pre></div></div>

<p>A benefit of the decay filter is that the intervals of selected frames are relatively easy to control. With fine tuning and ideal conditions, the decay filter can be described as a filter that prioritizes tight control on the interval between frames, and compromises for a pseudo interval maximum. The biggest flaw with the decay filter is that it compromises with an interval maximum, meaning it does not necessarily consider the local context of the value, and merely selects them based on their height. It is also not a full interval maximum filter because it selects frames based on an arbitrary decay cycle, meaning it does not necessarily select the highest value in a given interval.</p>

<p>The decay filter with threshold 100 and decay constant 0.5, applied to a 3 second demonstration video is shown in figure 10. The horizontal axis is the frame, and the vertical axis is the corresponding Laplacian variance. It is to be noted that an initial threshold of 100 is likely too high for this sample, but the equally large decay constant makes for a good demonstration of the decay filter’s interval consistency.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure10.svg" alt="figure10" title="figure10" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 10: Laplacian variance by frame overlayed with decay filter selection in red </div>
<p><br /></p>

<h3 id="324-moving-maximum-filter">3.2.4. Moving Maximum Filter</h3>

<p>Program 8 shows the implementation of the moving maximum filter. The moving maximum filter takes a block size as input, and moves this block throughout the array, always selecting the largest value in the block. Like the moving average, the moving maximum filter soothes a large discontinuous array into a relatively continuous array. In the case of the moving maximum filter, the individual Laplacian variance values are smoothed into a function that represents the local ceiling values.</p>

<div style="text-align: center;"> Program 8: “MovingMaximumFilter” function definition </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">MovingMaximumFilter</span><span class="p">(</span><span class="n">laplacians</span><span class="p">,</span> <span class="n">blockSize</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">array</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">laplacians</span><span class="p">)</span><span class="o">-</span><span class="n">blockSize</span><span class="p">):</span>
        <span class="n">splice</span> <span class="o">=</span> <span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">blockSize</span><span class="p">]</span>
        <span class="nb">max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">splice</span><span class="p">)</span>
        <span class="n">arg</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">splice</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">prev</span> <span class="o">!=</span> <span class="n">arg</span><span class="p">):</span> 
            <span class="n">array</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">arg</span><span class="p">,</span> <span class="nb">max</span><span class="p">])</span>
            <span class="n">prev</span> <span class="o">=</span> <span class="n">arg</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
</code></pre></div></div>

<p>Program 8 loops through the values of the array and finds the maximum value in a splice of length “blockSize” starting from the current index. If this value has not yet been selected, it is appended to an array which is retuned at the end of the function. Thus, for large block sizes such as 10, it is normal for the same point to be selected multiple times. The benefit of the moving maximum filter is that it is a full interval maximum filter, meaning that it always selects the highest value in a given interval. However, to reiterate, the highest value does not necessarily indicate the sharpest frame, and therefore it is necessary to pick a block size that strikes a fine balance between the locality necessary for making the variance values meaningful, and the generality that is necessary to create a meaningful distinction from the local maxima filter that will be covered next. In one sentence, the moving maximum filter carries over the decay filter’s constant interval property, but also implements a full interval maximum filter unlike the decay filter.</p>

<p>The same 3 second demonstration video was filtered through a moving maximum filter with the block size set to 10, and the results are shown in figure 11. It can be observed that the moving maximum filter generally selects ideal values, but there are areas where needless selections occur, and others where perfectly good candidates are ignored because of their higher neighbors.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure11.svg" alt="figure11" title="figure11" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 11: Laplacian variance by frame overlayed with moving maximum filter selections in red </div>
<p><br /></p>

<h3 id="325-local-maxima-filter">3.2.5. Local Maxima Filter</h3>

<p>Program 9 shows the implementation of the local maxima filter. If the two adjacent array elements are both less than the current array element, the value is selected.</p>

<div style="text-align: center;"> Program 9: “LocalMaximaFilter” function definition </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LocalMaximaFilter</span><span class="p">(</span><span class="n">laplacians</span><span class="p">):</span>
    <span class="n">array</span> <span class="o">=</span>  <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">laplacians</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;</span><span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">and</span> <span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;</span><span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">array</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">i</span><span class="p">,</span> <span class="n">laplacians</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
</code></pre></div></div>

<p>The local maxima filter is likely to be the best method for this application because it follows the principles of auto focus in smartphone videos. Video auto focus in many smartphone cameras rely on passive feedback to constantly adjust the focus based on the previous frame, and thus include an inherent latency. In most videos, motion of an object or motion of the camera induces a pattern of, relatively in focus, relatively out of focus, relatively in focus, and so on… An example of this effect is shown in figure 12, where three adjacent frames and their Laplacian variances are displayed.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure12.png" alt="figure12" title="figure12" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 12: Auto focus for video has the tendency to alternate between focused and non-focused frames </div>
<p><br /></p>

<p>The benefit of the local maxima filter is its ability to select all the points in relative focus. However, selecting all points relative to their neighbors can simultaneously be hazardous as very blurry photos that are locally non-blurry can easily be selected on accident.
The 3 second demonstration video was filtered through the local maxima filter and the results are shown in figure 13. It can be observed that several points with insignificant local maxima have also been selected.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure13.svg" alt="figure13" title="figure13" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 13: Laplacian variance by frame overlayed with local maxima filter selections in red </div>
<p><br /></p>

<h2 id="33-comparing-the-results">3.3. Comparing the Results</h2>

<p>This section shows the results of a comparison between frames that were selected by the local maxima filter and those that were not selected. Figure 14 shows 4 frames that were selected from a longer 45 second video, and Figure 15 shows 4 frames that were not selected from the same video. Samples were taken randomly from the same frame range.</p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure14.png" alt="figure14" title="figure14" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 14: frames selected by the local maxima filter  </div>
<p><br /></p>

<p><img src="/assets/2025-5-28-3D-Reconstruction/figure15.png" alt="figure15" title="figure15" class="img-large-centered" /></p>
<div style="text-align: center;"> Figure 15: frames not selected by the local maxima filter </div>
<p><br /></p>

<p>It can be noted with some close inspection that the frames that were not selected by the local maxima filter are generally all blurry, while those that were selected by the filter can occasionally be blurry. It can be concluded from these results that the local maxima filter is relatively not selective, resulting in a tendency for false positives. On the other hand, the highly non-selective nature of the filter ensures that almost all of the non-blurry frames are indeed selected. This is a good result, as applying a second filter to further refine the results and discard the false positives becomes a possibility.
 </p>
<h1 id="4-conclusion">4. Conclusion</h1>

<p>In conclusion, as the amount and the quality of the information we share with the world continues to change and grow, the ways in which we convey them, the information medium, is also expected to change and grow. A direct consequence of this is that the demand for a means to convert between the real world and the virtual world increases rapidly. One way to potentially meet this demand is to introduce new technologies that increase the accessibility of 3D reconstruction models. This report suggested a new approach to select relatively good frames from a smartphone video, and detailed an implementation of the OpenCV camera calibration model which is a critical component of the 3D reconstruction pipeline.</p>

<h1 id="5-references">5. References</h1>

<p>[1]. MathWorks, “カメラキャリブレーションとは”, https://jp.mathworks.com/help/vision/ug/camera-calibration.html, Accessed: 7/2/24</p>

<p>[2]. Stanford University, “Homogeneous Coordinates”, https://ai.stanford.edu/~birch/projective/node4.html, Accessed: 6/21/24</p>

<p>[3]. OpenCV, “Camera Calibration and 3D Reconstruction”, https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html, Accessed: 6/21/24</p>

<p>[4]. Mark Hadley Jones, “Calibration Checkerboard Collection”, https://markhedleyjones.com/projects/calibration-checkerboard-collection, referenced 5/10/2024</p>

<p>[5]. OpenCV, “Camera Calibration”, https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html, Accessed: 6/21/24</p>

<p>[6]. OpenCV, “Image Filtering”, https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6, Accessed: 8/2/24</p>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#projects" class="page__taxonomy-item p-category" rel="tag">projects</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-05-28T00:00:00+09:00">May 28, 2025</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?text=%E6%96%87%E5%8C%96%E9%81%BA%E7%94%A3%E3%81%AE%E4%BF%9D%E5%AD%98%E3%81%A8%E5%85%B1%E6%9C%89%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%EF%BC%93%E6%AC%A1%E5%85%83%E5%BE%A9%E5%85%83%EF%BC%88%E8%8B%B1%E8%AA%9E%E7%89%88%EF%BC%89-+%E5%85%B1%E5%90%8C%E7%A0%94%E7%A9%B6%20http%3A%2F%2Flocalhost%3A4000%2Fprojects%2F3D-Reconstruction%2F" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fprojects%2F3D-Reconstruction%2F" class="btn btn--facebook" aria-label="Share on Facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook">
    <i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/projects/3D-Reconstruction/" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

  <a href="https://bsky.app/intent/compose?text=%E6%96%87%E5%8C%96%E9%81%BA%E7%94%A3%E3%81%AE%E4%BF%9D%E5%AD%98%E3%81%A8%E5%85%B1%E6%9C%89%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%EF%BC%93%E6%AC%A1%E5%85%83%E5%BE%A9%E5%85%83%EF%BC%88%E8%8B%B1%E8%AA%9E%E7%89%88%EF%BC%89-+%E5%85%B1%E5%90%8C%E7%A0%94%E7%A9%B6%20http%3A%2F%2Flocalhost%3A4000%2Fprojects%2F3D-Reconstruction%2F" class="btn btn--bluesky" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Bluesky">
    <i class="fab fa-fw fa-bluesky" aria-hidden="true"></i><span> Bluesky</span>
  </a>
</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/projects/C89-Parser/" class="pagination--pager" title="C89コンパイラのパーサをCで実装">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You May Also Enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/projects/Path-Planner/" rel="permalink">差動駆動およびホロノミックロボット向け経路計画インターフェース
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">このプロジェクトでは、差動駆動およびホロノミック Vex ロボットの滑らかな経路生成における Cubic Hermite Spline の応用について詳述します。経路生成は Desmos の使用により簡略化されています。また、本プロジェクトでは、初速度と最終速度、初期加速度と最終加速度、移動距離を与えたときの速度...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/projects/C89-Parser/" rel="permalink">C89コンパイラのパーサをCで実装
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">プログラムに携わるエンジニアとしてコンパイラは当然のように使う道具である。ただし、その裏には人間の意思を機械が実行できる指示に変換するための奥深い技術がある。また、コンパイラを勉強することによって、コンパイルエラーの処理能力が強化されたり、その言語の理解度が深まるといった利点が挙げられる。当然のように使う道具への...</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']], // Tells MathJax to look for $...$ and \(...\)
    displayMath: [['$$','$$'], ['\\[','\\]']], // Standard display math
    processEscapes: true, // Allows for escaping special characters
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // Important to avoid conflicts
  },
  TeX: {
    equationNumbers: { autoNumber: "AMS" } // Optional: For AMS style equation numbering
  }
});
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<link rel="stylesheet" href="/assets/css/custom.css">
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://github.com/Meilan39" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://jp.linkedin.com/in/meilan-muto-3b8a01355" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a href="http://localhost:4000">Meilan Muto Portfolio</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









  </body>
</html>
